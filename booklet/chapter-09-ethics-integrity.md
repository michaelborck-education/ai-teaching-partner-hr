# Chapter 9: The Ethics & Integrity Question

## The Conversation You Must Have

If you implement any of the ideas in this booklet, you will have this conversation—with students, with colleagues, possibly with administrators:

**"Aren't you just teaching students to cheat?"**

This chapter gives you the framework, language, and evidence to respond confidently. More importantly, it helps you position AI integration not as an academic integrity *problem*, but as an academic integrity *opportunity*—a chance to teach professional ethics and responsible technology use.

---

## Reframing the Question

The traditional framing:
> "How do we prevent students from using AI inappropriately?"

The professional framing:
> "How do we teach students to use AI responsibly in their HR careers?"

**The shift matters.**

The first framing treats AI as a threat to be controlled. The second treats AI literacy as a learning objective to be developed.

As an HR educator, you're not preparing students for a world without AI. You're preparing them for a world where AI tools will:
- Screen resumes and identify candidates
- Draft employment contracts and policies
- Analyse workforce data and predict turnover
- Generate interview questions and assessment criteria
- Summarize complex legislation and case law

Your graduates will use these tools. The question is: **Will they use them competently and ethically, or incompetently and recklessly?**

That's what this chapter is about.

---

## The Three-Part Framework for Ethical AI Use

This framework works for talking to students, colleagues, and administrators. It has three components:

### 1. Transparency (Not Prohibition)

**The principle:** Make AI use explicit, expected, and assessable rather than hidden and policed.

**In practice:**
- Tell students exactly when and how they can use AI
- Provide the prompts and tools yourself
- Assess their *use* of AI, not their *avoidance* of AI
- Reward students who identify AI's errors and limitations

**Why this builds integrity:**
When AI use is transparent, students learn to use it openly and responsibly. When it's prohibited, students use it secretly and don't develop critical oversight skills.

### 2. Critical Oversight (Not Blind Reliance)

**The principle:** Teach students that AI is a tool requiring human judgment, not an authority to be trusted.

**In practice:**
- Design assignments where students must critique or override AI outputs
- Require students to identify what AI gets wrong
- Grade students on their ability to improve on AI suggestions
- Show examples of AI failures (bias, errors, oversimplification)

**Why this builds integrity:**
Students learn that using AI thoughtfully is harder than avoiding it. They develop the professional habit of verification and critical thinking.

### 3. Professional Relevance (Not Academic Abstraction)

**The principle:** Connect AI use in coursework to AI use in professional practice.

**In practice:**
- Frame assignments as professional scenarios: "You're the HR manager using AI to draft a policy..."
- Discuss workplace AI ethics: "What happens if your AI resume screening tool discriminates?"
- Teach governance: "Who is accountable when AI-assisted decisions go wrong?"
- Include AI literacy as a stated learning objective in your unit outline

**Why this builds integrity:**
When students see AI use as professional skill development rather than academic shortcut, they engage differently. They're not "cheating the system"—they're practicing for their careers.

---

## Student-Facing Language: Setting Expectations

You need clear, direct communication about AI use. Here's a model you can adapt:

### Example: Unit Outline AI Policy Statement

```
ARTIFICIAL INTELLIGENCE USE IN THIS UNIT

In professional HR practice, you will use AI tools to support decision-making,
analysis, and communication. This unit teaches you to use AI responsibly and
critically.

WHEN AI USE IS EXPECTED:
- Assignment 2 (Conversation Simulation): You will interact with an AI persona
  and submit transcripts
- Assignment 3 (Self-Assessment): You will use the provided AI critique prompt
  to assess your draft before submission

WHEN AI USE IS PERMITTED:
- Brainstorming ideas for case study analysis
- Generating practice questions for exam preparation
- Checking grammar and clarity in written work
- Exploring concepts you don't fully understand yet

WHEN AI USE IS NOT PERMITTED:
- Final exam (closed book, no technology)
- Any assignment where instructions explicitly state "no AI tools"

WHAT YOU MUST DO WHEN USING AI:
- Use it as a tool that supports YOUR thinking, not replaces it
- Critically evaluate AI outputs—don't assume they're correct
- Be able to explain and justify any AI-assisted work in your own words
- Acknowledge AI use where required (e.g., "I used ChatGPT to generate
  initial interview questions, which I then revised based on...")

ACADEMIC INTEGRITY EXPECTATIONS:
Using AI inappropriately (e.g., submitting AI-generated work as your own
without critical engagement) is academic misconduct, just like plagiarism.

If you're ever unsure whether your AI use is appropriate, ask before
submitting. I'm here to help you learn to use these tools well.
```

### Example: First-Day Class Discussion

**What to say:**

> "Let's talk about AI. Some of you are probably already using ChatGPT or similar tools. Some of you are worried that using AI is cheating. Some of you are wondering if I'm going to try to detect and punish AI use.
>
> Here's my position: **AI tools exist, and you'll use them in HR jobs. My job is to teach you to use them wisely and ethically.**
>
> In this unit, we'll use AI openly in some assignments. You'll learn when AI is helpful, when it's risky, and when human judgment must override AI recommendations. That's a professional skill.
>
> I'm not interested in playing 'gotcha' with AI detection software. I'm interested in whether you can think critically, justify your decisions, and demonstrate competent HR practice. If you can do that with AI assistance, great. If you use AI to avoid thinking, I'll know—because your work won't demonstrate understanding.
>
> Questions or concerns about this approach?"

**Why this works:**
- Sets a clear, positive tone
- Positions you as a guide, not a cop
- Acknowledges student anxiety
- Makes professional relevance explicit
- Invites dialogue

---

## Designing "Integrity-Resistant" Assignments

Some assignments are easier to misuse with AI than others. Here's how to design assessments that are inherently resistant to misuse:

### Principle 1: Assess Process, Not Just Product

**Vulnerable design:** "Write a 1500-word essay analysing a workplace conflict."
- Student can paste this into AI and submit the output

**Integrity-resistant design:** "Conduct a simulated investigation interview (submit transcript), then audit your own process against procedural fairness criteria."
- Student must engage in real-time conversation (can't be pre-written)
- Assessment focuses on methodology visible in transcript
- Self-audit requires metacognitive engagement

### Principle 2: Require Evidence of Thinking

**Vulnerable design:** "Recommend a solution to this HR problem."
- AI can generate a plausible recommendation

**Integrity-resistant design:** "AI generated three solutions to this problem [provide them]. Critique each option, identify which one is best and why, and explain what the AI got wrong."
- Student must think beyond what AI provided
- Requires critical evaluation, not just generation
- Makes AI outputs the starting point, not the end point

### Principle 3: Make Personal Context Essential

**Vulnerable design:** "Analyse the pros and cons of flexible work policies."
- Generic question AI can answer generally

**Integrity-resistant design:** "Based on your conversation with the AI employee 'Jamie' (from your Week 4 simulation), analyse what flexible work approach would address Jamie's specific situation while meeting organisational needs."
- Requires integration of previous personalised work
- Context is unique to each student
- Generic AI response won't fit

### Principle 4: Assess Revision and Iteration

**Vulnerable design:** Submit final work only
- No visibility into how it was created

**Integrity-resistant design:** Submit first draft, AI feedback received, revised draft, and reflection on changes made
- Process is visible and assessable
- Shows learning trajectory
- Difficult to fake iterative improvement

### Principle 5: Require Justification of Choices

**Vulnerable design:** "Create a recruitment interview guide."
- AI can generate a complete guide

**Integrity-resistant design:** "Create an interview guide. For each question, justify why you chose it, what competency it targets, and what poor response would sound like. Identify two questions the AI generated that you rejected and explain why they were inadequate."
- Requires deep understanding, not just production
- Student must demonstrate judgment beyond AI capability
- Reveals whether they understand what they're submitting

---

## Red Flags for AI Misuse (And How to Address Them)

Even with well-designed assignments, some students will try to misuse AI. Here's how to identify and respond:

### Red Flag 1: Sudden Quality Shift

**What you see:** Student whose previous work was weak suddenly submits sophisticated analysis.

**Response approach:**
- **Don't immediately accuse.** There could be legitimate reasons (they got help from writing center, they finally understood the concept, etc.)
- **Ask questions:** "Your analysis has improved significantly. Can you walk me through your thinking process on this particular section?"
- **Request elaboration:** "This point about organisational justice theory is interesting. Can you explain how you see it applying to this specific scenario?"

**If genuine learning:** They can explain their thinking.
**If inappropriate AI use:** They struggle to explain or elaborate.

### Red Flag 2: Work That Doesn't Match Assignment Context

**What you see:** Student used generic AI response that doesn't fit the specific scenario or constraints you provided.

**Example:** Assignment asked for Australian employment law context, student submitted response referencing US legislation.

**Response approach:**
- **Point out the mismatch:** "I notice you've referenced Title VII of the Civil Rights Act, but this assignment requires Australian context. Can you explain how this applies to our scenario?"
- **Provide opportunity to revise:** "I think you may have used a resource that wasn't contextually appropriate. Please resubmit with correct jurisdictional references."

**Teaching moment:** Use this to discuss the importance of contextual verification when using AI tools professionally.

### Red Flag 3: No Evidence of Process in Process-Based Assessment

**What you see:** Student submitted required components but shows no genuine engagement (e.g., self-audit identifies no mistakes, reflection is superficial).

**Response approach:**
- **Return for revision:** "Your self-audit suggests your performance was perfect. Reflective practice requires identifying areas for growth. Please resubmit with honest self-assessment."
- **Offer guidance:** "Everyone makes mistakes in complex HR conversations. Look specifically at moments where the employee seemed frustrated or defensive—what might you have done differently?"

**Teaching moment:** Explain that honest self-assessment is more valuable than false perfection.

### Red Flag 4: Can't Explain or Defend Work in Person

**What you see:** High-quality written submission, but student can't discuss it in office hours or oral follow-up.

**Response approach:**
- **For high-stakes situations:** Schedule a brief oral examination: "I'd like to discuss your assignment. Can you walk me through your main recommendation and why you chose it?"
- **Frame it as learning:** "I was impressed by your analysis. I'd love to hear more about your thinking process."

**If inappropriate use is confirmed:**
- Follow university academic misconduct procedures
- Use it as a teaching moment about professional accountability

---

## Teaching AI Ethics Through HR Scenarios

One of the most powerful ways to address integrity is to make it a learning objective. Teach students to identify ethical problems with AI use *through HR scenarios*.

### Exercise 1: The Flawed AI Termination Memo

**Assignment:**

> "Use AI to draft a termination letter for an employee being dismissed for poor performance after a 60-day PIP.
>
> Then conduct an ethical audit:
> - What did the AI include that could create legal risk?
> - What did the AI omit that's legally required?
> - What tone or language choices are problematic?
> - How would you revise this to ensure procedural fairness?
>
> Your grade is based on how thoroughly you identify problems, not on the quality of AI's original output."

**What students learn:**
- AI can confidently generate legally dangerous content
- They must verify and correct AI outputs
- Professional accountability can't be delegated to AI

### Exercise 2: The Biased Resume Screening Tool

**Scenario:**

> "Your company uses an AI resume screening tool. You notice that it consistently ranks candidates from certain universities higher and flags career gaps as negative. Three rejected candidates have complained that the process seems unfair.
>
> As the HR manager:
> 1. What are the ethical concerns with this AI tool?
> 2. What's your legal risk?
> 3. Who is accountable for the AI's decisions—the vendor, the company, you personally?
> 4. What would you do to address this situation?"

**What students learn:**
- Algorithmic bias is a real professional issue
- Using AI doesn't eliminate human responsibility
- HR professionals must advocate for fair processes even when technology is involved

### Exercise 3: The Over-Reliance Problem

**Scenario:**

> "You used AI to analyse exit interview data and generate recommendations for reducing turnover. You presented the AI's recommendations to senior management, and they were implemented. Six months later, turnover has increased.
>
> Reflection questions:
> 1. What might the AI have missed in its analysis?
> 2. What was your professional responsibility before presenting AI recommendations?
> 3. How do you explain this outcome to management?
> 4. What does this teach you about using AI in strategic decision-making?"

**What students learn:**
- AI analysis isn't inherently correct
- Professional judgment can't be outsourced
- They're accountable for recommendations they present, regardless of how they were generated

---

## Responding to Colleague and Administrator Concerns

You may need to justify your approach to colleagues or administrators who are skeptical about AI integration.

### Concern: "This undermines academic standards"

**Response:**

> "Actually, it raises standards. I'm no longer testing whether students can recall information—I'm testing whether they can apply it in realistic, dynamic scenarios. I'm assessing higher-order thinking: critical evaluation, professional judgment, and ethical reasoning. These are harder to demonstrate than memorization."

### Concern: "How do you know they're learning anything?"

**Response:**

> "I assess their process, not just their final product. I can see their thinking in conversation transcripts, in their critiques of AI outputs, and in their reflective analysis. When students can identify what AI got wrong and explain why, they're demonstrating deep understanding."

### Concern: "This doesn't align with university academic integrity policies"

**Response:**

> "University policies typically prohibit *unacknowledged* or *uncritical* use of external sources. My approach makes AI use acknowledged and requires critical evaluation. Students aren't hiding AI use—they're demonstrating competent use. That's consistent with academic integrity principles, just applied to a new tool."

**Supporting evidence:**
- Many universities are updating policies to allow appropriate AI use
- Professional accreditation bodies are recognizing AI literacy as essential
- Employer expectations include ability to use AI tools responsibly

### Concern: "What if other lecturers don't agree?"

**Response:**

> "That's fine—pedagogical approaches can vary across units. I'm being transparent with students about expectations in *my* unit. If other lecturers prohibit AI use, students can follow those different expectations. Professional practice requires adapting to different contexts anyway—this models that."

---

## The Bigger Picture: AI Literacy as Graduate Capability

Position AI literacy as a graduate capability alongside communication, critical thinking, and ethical practice.

### What AI Literacy Means for HR Graduates

**Competent HR graduates should be able to:**

1. **Identify appropriate use cases**
   - When is AI helpful? (data analysis, initial drafts, generating options)
   - When is AI risky? (sensitive employee conversations, final legal decisions)
   - When is human judgment essential? (ethical dilemmas, complex interpersonal situations)

2. **Evaluate AI outputs critically**
   - Does this align with legal requirements?
   - Is this ethically sound?
   - What assumptions has the AI made?
   - What context is missing?

3. **Maintain accountability**
   - Understanding that using AI doesn't eliminate responsibility
   - Knowing when to verify AI recommendations with experts
   - Documenting decision-making processes

4. **Recognize bias and limitations**
   - Algorithmic bias in recruitment tools
   - Cultural insensitivity in AI-generated policies
   - Over-generalization of complex situations

**This is professional education, not just academic integrity management.**

---

## A Final Ethical Consideration

Here's a question to leave with:

**Is it ethical to graduate HR professionals who don't know how to use AI responsibly?**

When your graduates enter the workforce, they will encounter:
- AI-powered recruitment systems making hiring decisions
- Automated performance monitoring tools
- AI chatbots handling employee queries
- Algorithmic workforce management systems

If they don't understand how to evaluate these tools critically, advocate for fair processes, and identify when human oversight is essential, **they will cause harm**—not through malice, but through incompetence.

Your responsibility as an educator isn't to protect students from AI. It's to prepare them to be ethical, competent professionals in an AI-augmented world.

Teaching them to use AI transparently, critically, and responsibly in your course isn't lowering standards.

**It's fulfilling your educational duty.**

---

## Your Action Step

Before the Appendices, draft your own AI use statement for your next unit outline. Use the framework from this chapter:

1. **When AI use is expected** (specific assignments)
2. **When AI use is permitted** (general study support)
3. **When AI use is not permitted** (exams, specific constraints)
4. **What students must do** (critical engagement, acknowledgment)
5. **Academic integrity expectations** (consequences of misuse)

Write it in your own voice. Make it clear, direct, and positive.

Then review it against this question: **Would a student reading this understand how to use AI appropriately and why it matters for their professional development?**

---

**Next Section Preview:** The Appendices provide ready-to-use resources: a prompt library you can copy and adapt immediately, a one-hour workshop guide for introducing these ideas to colleagues, and a detailed alignment with Curtin University learning outcomes to show how AI integration supports existing educational goals.
